behaviors:
  PostalBehaviour:
    trainer_type: ppo
    summary_freq: 10000
    max_steps: 10000000
    keep_checkpoints: 5
    checkpoint_interval: 500000
    time_horizon: 64
    threaded: false
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
#      PPO-specific & POCA-specific
      beta: 5.0e-3
      epsilon: 0.2
      beta_schedule: linear # learning_rate_schedule
      epsilon_schedule: linear # learning_rate_schedule
      lambd: 0.95
      num_epoch: 3
      shared_critic: false
#      SAC-specific
#      buffer_init_steps: 0
#      init_entcoef: 1.0
#      save_replay_buffer: false
#      tau: 0.005
#      steps_per_update: 1
#      reward_signal_num_update: steps_per_update
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
      conditioning_type: hyper
#      memory:
#        memory_size: 128
#        sequence_length: 64
    reward_signals:
      extrinsic:
        strength: 1.0
        gamma: 0.99
#      curiosity:
#        strength: 0.5
#        gamma: 0.99
#        learning_rate: 3e-4
#      gail:
#         strength: 0.9
#         demo_path: Assets/Demonstrations/MoveToGoal_2.demo
#         gamma: 0.99
#         learning_rate: 3e-4
#         use_actions: false
#         use_vail: false
#      rnd:
#        strength: 1.0
#        gamma: 0.99
#        network_settings: 
#        learning_rate: 3e-4
#    behavioral_cloning:
#       strength: 1
#       demo_path: Assets/Demonstrations/MoveToGoal_2.demo
#       steps: 0
#       batch_size: batch_size
#       num_epoch: num_epoch
#       samples_per_update: 0
